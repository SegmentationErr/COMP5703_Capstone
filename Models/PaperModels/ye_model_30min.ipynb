{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from keras.layers import Dropout\n",
    "from keras.engine.sequential import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from tensorflow.keras.optimizers import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/30MIN_Full_Dataset.csv'\n",
    "\n",
    "data = pd.read_csv(data_path)\n",
    "data = data.drop(['timestamp'], axis=1)[5:]\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Price Data:'''\n",
    "# data_features = data.drop(['price_close', 'MACD', 'SMA', 'OBV', 'RSI', 'MFI', 'vader_pos', 'vader_neg', 'vader_neu', 'vader_compound', 'pos_count', 'neg_count', 'neu_count'], axis=1)\n",
    "\n",
    "'''Price Data + Technical Indicator:'''\n",
    "# data_features = data.drop(['price_close', 'vader_pos', 'vader_neg', 'vader_neu', 'vader_compound', 'pos_count', 'neg_count', 'neu_count'], axis=1)\n",
    "\n",
    "'''Price Data + Sentiment Indicator:'''\n",
    "# data_features = data.drop(['price_close', 'MACD', 'SMA', 'OBV', 'RSI', 'MFI', 'pos_count', 'neg_count', 'neu_count'], axis=1)\n",
    "\n",
    "'''Price Data + Technical Indicator + Sentiment Indicator:'''\n",
    "# data_features = data.drop(['price_close', 'pos_count', 'neg_count', 'neu_count'], axis=1)\n",
    "\n",
    "'''Price Data + Technical Indicator in 30 MIN data:'''\n",
    "# data_features = data.drop(['price_close', 'vader_pos', 'vader_neg', 'vader_neu', 'vader_compound', 'pos_count', 'neg_count', 'neu_count', 'pos_keywords_occur', 'neg_keywords_occur'], axis=1)\n",
    "\n",
    "'''Price Data + Selected Features'''\n",
    "data_features = data.drop(['price_close', 'MFI', 'vader_compound',\t'pos_count', 'neg_count', 'neu_count'], axis=1)\n",
    "\n",
    "data_labels = data['price_close']\n",
    "\n",
    "scaler_feature = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_label = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "feature = scaler_feature.fit_transform(data_features.to_numpy())\n",
    "label = scaler_label.fit_transform(data_labels.to_numpy().reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(f, l, delay):\n",
    "    feature, label = [], []\n",
    "\n",
    "    for i in range(len(f) - delay):\n",
    "        feature.append(f[i: i + delay])\n",
    "        label.append(l[i + delay-1])\n",
    "        \n",
    "    return np.array(feature), np.array(label).squeeze()\n",
    "\n",
    "window_size = 5\n",
    "\n",
    "feature, label = split_data(feature, label, window_size)\n",
    "\n",
    "train_size = int(0.8 * len(label))\n",
    "\n",
    "X_train, X_test = feature[:train_size], feature[train_size:]\n",
    "y_train, y_test = label[:train_size], label[train_size:]\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train X data into 5 datasets\n",
    "newarr = np.array_split(X_train, 5)\n",
    "X_train_1 = newarr[0] \n",
    "X_train_2 = newarr[1]\n",
    "X_train_3 = newarr[2]\n",
    "X_train_4 = newarr[3]\n",
    "X_train_5 = newarr[4]\n",
    "\n",
    "# split train y data into 5 datasets\n",
    "newarr_y = np.array_split(y_train, 5)\n",
    "y_train_1 = newarr_y[0] \n",
    "y_train_2 = newarr_y[1]\n",
    "y_train_3 = newarr_y[2]\n",
    "y_train_4 = newarr_y[3]\n",
    "y_train_5 = newarr_y[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset 1 （1-4）\n",
    "X_train_dataset_1 = X_train_1+X_train_2+X_train_3+X_train_4\n",
    "\n",
    "# Train dataset 2 （1-3，5）\n",
    "X_train_dataset_2 = X_train_1+X_train_2+X_train_3+X_train_5\n",
    "\n",
    "# Train dataset 3 （1，2，4，5）\n",
    "X_train_dataset_3 = X_train_1+X_train_2+X_train_4+X_train_5\n",
    "\n",
    "# Train dataset 4 （1，3-5）\n",
    "X_train_dataset_4 = X_train_1+X_train_3+X_train_4+X_train_5\n",
    "\n",
    "# Train dataset 5 （2-5）\n",
    "X_train_dataset_5 = X_train_2+X_train_3+X_train_4+X_train_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset 1 （1-4）\n",
    "y_train_dataset_1 = y_train_1+y_train_2+y_train_3+y_train_4\n",
    "\n",
    "# Test dataset 2 （1-3，5）\n",
    "y_train_dataset_2 = y_train_1+y_train_2+y_train_3+y_train_5\n",
    "\n",
    "# Test dataset 3 （1，2，4，5）\n",
    "y_train_dataset_3 = y_train_1+y_train_2+y_train_4+y_train_5\n",
    "\n",
    "# Test dataset 4 （1，3-5）\n",
    "y_train_dataset_4 = y_train_1+y_train_3+y_train_4+y_train_5\n",
    "\n",
    "# Test dataset 5 （2-5）\n",
    "y_train_dataset_5 = y_train_2+y_train_3+y_train_4+y_train_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_datasets = [X_train_dataset_1,X_train_dataset_2,X_train_dataset_3,X_train_dataset_4,X_train_dataset_5]\n",
    "y_train_datasets = [y_train_dataset_1,y_train_dataset_2,y_train_dataset_3,y_train_dataset_4,y_train_dataset_5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nGRU = 5\n",
    "nLSTM = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRUs = [\n",
    "    keras.Sequential([\n",
    "        layers.GRU(2048),\n",
    "        layers.Dense(1024),\n",
    "        layers.Dense(1)\n",
    "    ]) for i in range(nGRU)\n",
    "]\n",
    "\n",
    "\n",
    "LSTMs = [\n",
    "    keras.Sequential([\n",
    "        layers.SimpleRNN(512),\n",
    "        layers.Dense(512),\n",
    "        layers.Dense(1)\n",
    "    ]) for i in range(nLSTM)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, X, y in zip(GRUs, X_train_datasets, y_train_datasets):\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.00005),\n",
    "                metrics=[keras.metrics.RootMeanSquaredError()]\n",
    "                )\n",
    "    history = model.fit(X, y,\n",
    "                        epochs=30,\n",
    "                        batch_size=4,\n",
    "                        shuffle=True\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, X, y in zip(LSTMs, X_train_datasets, y_train_datasets):\n",
    "    model.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=1e-3), metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    history = model.fit(X, y, \n",
    "                        batch_size=16, \n",
    "                        epochs=50, \n",
    "                        shuffle=False,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_GRU = []\n",
    "pred_LSTM = []\n",
    "for gru, lstm, data in zip(GRUs, LSTMs, newarr[::-1]):\n",
    "    pred_GRU = gru.predict(data).tolist() + pred_GRU\n",
    "    pred_LSTM = lstm.predict(data).tolist() + pred_LSTM\n",
    "    \n",
    "pred_GRU = np.array(pred_GRU)\n",
    "pred_LSTM = np.array(pred_LSTM)\n",
    "\n",
    "X_tmp = np.concatenate((pred_GRU, pred_LSTM), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prices = scaler_label.inverse_transform(np.mean(np.array(pred_GRU), axis=1).reshape(-1, 1))\n",
    "true_prices = scaler_label.inverse_transform(y_train.reshape(-1,1))\n",
    "\n",
    "mse = mean_squared_error(true_prices, pred_prices)\n",
    "rmse = mean_squared_error(true_prices, pred_prices, squared=False)\n",
    "mae = mean_absolute_error(true_prices, pred_prices)\n",
    "\n",
    "print(mse, rmse, mae)\n",
    "\n",
    "pred_prices = scaler_label.inverse_transform(np.mean(np.array(pred_LSTM), axis=1).reshape(-1, 1))\n",
    "\n",
    "mse = mean_squared_error(true_prices, pred_prices)\n",
    "rmse = mean_squared_error(true_prices, pred_prices, squared=False)\n",
    "mae = mean_absolute_error(true_prices, pred_prices)\n",
    "\n",
    "print(mse, rmse, mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(26, 10))\n",
    "\n",
    "plt.plot(pred_GRU, label='GRU')\n",
    "plt.plot(pred_LSTM, label='LSTM')\n",
    "plt.plot(y_train, label='Ground Truth')\n",
    "plt.legend()\n",
    "plt.title('Predictions without Sentiment Analysis')\n",
    "\n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "model = Sequential([Dense(1)])\n",
    "\n",
    "model.compile(optimizer= Adam(learning_rate=5e-4), loss='mse', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_tmp, y_train, batch_size = batch_size, epochs=50, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_GRU = [model.predict(X_test) for model in GRUs]\n",
    "pred_LSTM = [model.predict(X_test) for model in LSTMs]\n",
    "\n",
    "pred_GRU = np.mean(np.array(pred_GRU), axis=0)\n",
    "pred_LSTM = np.mean(np.array(pred_LSTM), axis=0)\n",
    "\n",
    "X_tmp2 = np.concatenate((pred_GRU, pred_LSTM), axis=1).squeeze()\n",
    "\n",
    "prediction = model.predict(X_tmp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prices = scaler_label.inverse_transform(prediction)\n",
    "true_prices = scaler_label.inverse_transform(y_test.reshape(-1,1))\n",
    "\n",
    "mse = mean_squared_error(true_prices, pred_prices)\n",
    "rmse = mean_squared_error(true_prices, pred_prices, squared=False)\n",
    "mae = mean_absolute_error(true_prices, pred_prices)\n",
    "\n",
    "metrics = '\\n'.join((\n",
    "    '$MSE=%f$' % (mse),\n",
    "    '$RMSE=%f$' % (rmse),\n",
    "    '$MAE=%f$' % (mae)\n",
    "))\n",
    "\n",
    "plt.figure(figsize=(26, 10))\n",
    "plt.plot(pred_prices, label='Prediction Price')\n",
    "plt.plot(true_prices, label='Ground Truth')\n",
    "plt.legend()\n",
    "plt.title('Predictions for Meta-Model (1 day)')\n",
    "\n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "plt.text(0, 30000, metrics, fontsize=14,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "# plt.savefig('no_senti.png', dpi=500)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
